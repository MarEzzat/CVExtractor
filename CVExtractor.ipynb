{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhL0a_X8cTpY"
      },
      "source": [
        "# **Import Libs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZmhdYrgGnYfz",
        "outputId": "f93b0f16-d1cc-4568-c286-9c73f2055729"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Gc7myodcSft"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "from google.colab import files\n",
        "from transformers import pipeline\n",
        "import re\n",
        "import logging\n",
        "# Suppress BERT model warnings\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEQOB_evcvoz"
      },
      "source": [
        "#  **The Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkA9yA83dDeY"
      },
      "outputs": [],
      "source": [
        "# Upload PDF\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "9pzixR5pY85G",
        "outputId": "e3eef3c9-10e8-4373-f030-029a7991fed2"
      },
      "outputs": [],
      "source": [
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Unable to process {pdf_path}\\nerror = {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Load pre-trained BERT NER model silently\n",
        "nlp = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
        "\n",
        "# Process the PDF\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "if pdf_text is None:\n",
        "    print(\"Failed to extract text from PDF\")\n",
        "else:\n",
        "    # Run BERT NER on the text\n",
        "    ner_results = nlp(pdf_text)\n",
        "\n",
        "    # Desired entities\n",
        "    desired_entities = {\"NAME\", \"PHONE\", \"EDUCATION_LOC\", \"EDUCATION_YEAR\"}\n",
        "    d = {}\n",
        "\n",
        "    # Extract NAME (PERSON) from BERT with better reconstruction\n",
        "    name_parts = []\n",
        "    for i, entity in enumerate(ner_results):\n",
        "        if entity[\"entity\"].startswith(\"B-PER\") or entity[\"entity\"].startswith(\"I-PER\"):\n",
        "            name_parts.append(entity[\"word\"])\n",
        "        elif name_parts and not entity[\"entity\"].startswith(\"I-PER\"):\n",
        "            full_name = \" \".join(name_parts).replace(\" ##\", \"\").strip()\n",
        "            if len(full_name.split()) >= 2:  # Multi-word name\n",
        "                d[\"NAME\"] = [full_name]\n",
        "                break\n",
        "            name_parts = []\n",
        "    if name_parts and \"NAME\" not in d:  # Handle name at end\n",
        "        full_name = \" \".join(name_parts).replace(\" ##\", \"\").strip()\n",
        "        if len(full_name.split()) >= 2:\n",
        "            d[\"NAME\"] = [full_name]\n",
        "    if \"NAME\" not in d:  # Fallback: First line\n",
        "        first_line = pdf_text.split('\\n')[0].strip()\n",
        "        if len(first_line.split()) >= 2 and not any(x in first_line.lower() for x in [\"phone\", \"tel\", \"mobile\", \"contact\"]):\n",
        "            d[\"NAME\"] = [first_line]\n",
        "\n",
        "    # Extract PHONE (broader regex)\n",
        "    phone_match = re.search(r'(Phone|Tel|Mobile|Contact):?\\s*([\\+0-9\\-\\(\\)\\s]{8,})', pdf_text, re.IGNORECASE)\n",
        "    if phone_match:\n",
        "        d[\"PHONE\"] = [phone_match.group(2).strip()]\n",
        "    else:\n",
        "        phone_match = re.search(r'[\\+0-9\\-\\(\\)\\s]{8,}', pdf_text)\n",
        "        if phone_match:\n",
        "            d[\"PHONE\"] = [phone_match.group(0).strip()]\n",
        "\n",
        "    # Extract EDUCATION_LOC (robust for both resumes)\n",
        "    edu_loc_parts = []\n",
        "    for i, entity in enumerate(ner_results):\n",
        "        if entity[\"entity\"].startswith(\"B-ORG\") or entity[\"entity\"].startswith(\"I-ORG\") or \\\n",
        "           entity[\"entity\"].startswith(\"B-LOC\") or entity[\"entity\"].startswith(\"I-LOC\"):\n",
        "            edu_loc_parts.append(entity[\"word\"])\n",
        "            if any(keyword in entity[\"word\"].lower() for keyword in [\"university\", \"college\", \"institute\"]):\n",
        "                full_loc = \" \".join(edu_loc_parts).replace(\" ##\", \"\").strip()\n",
        "                # Clean up to just the institution name\n",
        "                match = re.search(r'(?:faculty of|at)?\\s*([\\w\\s]+?(?:university|college|institute))\\b', full_loc, re.IGNORECASE)\n",
        "                if match:\n",
        "                    d[\"EDUCATION_LOC\"] = [match.group(1).strip()]\n",
        "                else:\n",
        "                    d[\"EDUCATION_LOC\"] = [full_loc]\n",
        "                break\n",
        "            elif len(edu_loc_parts) > 3:  # Reset if too many unrelated parts\n",
        "                edu_loc_parts = []\n",
        "    if \"EDUCATION_LOC\" not in d:  # Fallback: Search lines with education keywords\n",
        "        for line in pdf_text.split('\\n'):\n",
        "            if any(keyword in line.lower() for keyword in [\"university\", \"college\", \"institute\", \"faculty\"]):\n",
        "                match = re.search(r'(faculty of|at)?\\s*([\\w\\s]+?(?:university|college|institute))(?:,|\\s*\\d{4}|$)', line, re.IGNORECASE)\n",
        "                if match:\n",
        "                    d[\"EDUCATION_LOC\"] = [match.group(2).strip()]\n",
        "                else:\n",
        "                    for keyword in [\"university\", \"college\", \"institute\"]:\n",
        "                        if keyword in line.lower():\n",
        "                            start = line.lower().index(keyword) - 20\n",
        "                            end = line.lower().index(keyword) + len(keyword) + 20\n",
        "                            d[\"EDUCATION_LOC\"] = [line[start:end].strip()]\n",
        "                            break\n",
        "                break\n",
        "\n",
        "    # Extract EDUCATION_YEAR (regex for years between 2000-2025)\n",
        "    year_match = re.search(r'\\b(20[0-2][0-5])\\b', pdf_text)\n",
        "    if year_match:\n",
        "        d[\"EDUCATION_YEAR\"] = [year_match.group(1)]\n",
        "\n",
        "    # Write to file\n",
        "    with open(\"resume_entities.txt\", \"w\") as f:\n",
        "        for label in sorted(desired_entities):\n",
        "            if label in d:\n",
        "                f.write(f\"{label}:\\n\")\n",
        "                for text in set(d[label]):\n",
        "                    cleaned_text = text.replace('\\n', '')\n",
        "                    f.write(f\"{cleaned_text}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "    print(\"Entities saved to resume_entities.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UufsgvlbX5M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
